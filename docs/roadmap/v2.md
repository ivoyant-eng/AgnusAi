# v2 Roadmap â€” Deeper Code Intelligence

The following features extend AgnusAI beyond diff-level reviews into full codebase understanding.

## Priority Overview

> Research benchmark: even the best AI code review tools score only **39â€“49% F1** on real PRs. Up to **40% of AI alerts are ignored** by developers once trust is lost. The items below address the root causes.

| Priority | Feature | Impact | Effort | Status |
|----------|---------|--------|--------|--------|
| **P1** | Precision-First Output (confidence threshold) | ğŸ”´ High | ğŸŸ¢ Low | âœ… Complete |
| **P1** | Static Analysis Layer (deterministic signal-first) | ğŸ”´ High | ğŸŸ¡ Medium | âœ… Complete |
| **P1** | Institutional Memory (team decision store) | ğŸ”´ High | ğŸŸ¡ Medium | ğŸ”„ In Progress |
| **P2** | Intent / Ticket Verification | ğŸ”´ High | ğŸŸ¡ Medium | Not Started |
| **P2** | TypeScript Type Checking | ğŸŸ¡ Medium | ğŸŸ¡ Medium | Not Started |
| **P2** | Codebase Embeddings | ğŸ”´ High | ğŸ”´ High | Not Started |
| **P2** | Production Metadata Injection | ğŸ”´ High | ğŸ”´ High | Not Started |
| **P3** | Multi-language LSP | ğŸŸ¡ Medium | ğŸ”´ High | Not Started |
| **P3** | Cross-Repository Impact Analysis | ğŸ”´ High | ğŸ”´ High | Not Started |

---

## P1: Precision-First Output

**The problem:** All current AI review tools â€” including CodeRabbit â€” default to high-recall output: comment on everything, let the developer sort it out. This creates alert fatigue. In one documented evaluation, CodeRabbit posted 10 comments on a 493-line PR â€” all noise, zero signal. Once trust is lost, developers dismiss all output including the real finds.

**The fix:** Introduce a confidence threshold. If no comment clears the threshold, post nothing. "This PR looks good" is a valid, valuable output.

```
For each candidate comment:
  score = confidence(comment, diff_context)
  if score < THRESHOLD â†’ discard silently
  else â†’ post

If no comments remain â†’ post "No significant issues found"
```

**Implementation approach:**
- Add a `confidence` field to `ReviewComment` (0.0â€“1.0)
- Instruct the LLM to self-score each comment in the output
- Apply a configurable threshold (default `0.7`) in `filterComments()`
- Track precision metrics over time: if a comment type is repeatedly dismissed, lower its confidence floor

**Key metric:** Signal ratio (actionable comments / total comments) should target > 60%. Most tools today are far below this.

---

## P1: Static Analysis Layer (Signal-First)

**The problem:** LLM-only reviewers hallucinate findings and miss deterministic bugs. CodeRabbit's advantage is running 40+ SAST/linter tools and using the LLM to filter and explain their output â€” not to invent findings. AgnusAI currently does the opposite: pure LLM generation with no deterministic ground truth.

**The fix:** Run static analysis tools first; feed their output to the LLM for prioritisation and explanation. The LLM's job becomes signal triage, not signal generation.

```
Diff
 â”‚
 â–¼
Static Analysis Layer
 â”œâ”€â”€ ESLint / Biome (JS/TS)
 â”œâ”€â”€ Semgrep rules (security patterns)
 â”œâ”€â”€ TypeScript compiler diagnostics
 â””â”€â”€ Custom rule set
 â”‚
 â–¼
LLM receives: diff + static analysis findings
"Filter these findings, explain only the actionable ones,
 and add any issues the static tools cannot detect."
 â”‚
 â–¼
Output: deduplicated, explained, ranked findings
```

**Why this works:** Static analysis findings are deterministic â€” same input always produces the same output. This eliminates the non-determinism problem and grounds the LLM in verified signals rather than guesses.

**Implementation approach:**
- Add an optional `staticAnalysis` step in `ReviewEngine` before LLM call
- Start with TypeScript compiler (`tsc --noEmit`) â€” zero new dependencies
- Add Semgrep as optional integration (security-focused teams)
- Inject findings into the review prompt as structured pre-context

---

## P1: Institutional Memory

**The problem:** A team spends 45 minutes debating a locking strategy, reaches a decision. Next PR touching the same code gets an AI comment re-opening the same debate. AI tools have no access to prior review thread conclusions. CodeRabbit's "learning" only suppresses dismissed comment _types_ â€” it doesn't store reasoning or enforce specific decisions.

**The fix:** Store team decisions from review threads as structured rules and replay them as enforcement on future PRs.

```
Review thread: "use pessimistic locking in the payments service"
 â”‚
 â–¼
Saved to memory store:
{
  "rule": "payments service must use pessimistic locking",
  "files": ["src/payments/**"],
  "decided": "2025-01-15",
  "thread_url": "...",
  "enforced_since": "SHA abc1234"
}
 â”‚
 â–¼
Future PR touches src/payments/transaction.ts
 â”‚
 â–¼
Memory rules injected into review prompt as team-specific constraints
LLM enforces them â€” and cites the original decision
```

**Storage options:** Persist rules in a `~/.pr-review/memory/` directory as YAML files (no database required); index by file glob pattern.

**Two input paths:**
1. **Manual:** Team adds rules explicitly via a CLI command (`agnus remember "..."`)
2. **Semi-automatic:** After a long review thread, the bot summarises the conclusion and asks "Should I remember this decision for future reviews?"

---

## P2: Intent / Ticket Verification

**The problem:** No AI reviewer can answer the most important question: *does this code actually do what it was supposed to do?* They review code quality, not whether the right thing was built. Academic research and HN discussions consistently cite this as the highest-value gap: "It didn't catch that the whole thing could've been replaced with a Postgres query."

**The fix:** Fetch the linked ticket/issue, extract acceptance criteria, and verify the implementation addresses them.

```
PR description links to: JIRA-1234 / GitHub Issue #89
 â”‚
 â–¼
Fetch ticket:
  title, description, acceptance criteria, labels
 â”‚
 â–¼
Inject into review prompt:
  "This PR claims to implement: [ticket description]
   Acceptance criteria: [list]
   Verify the diff satisfies each criterion.
   Flag any criterion that appears unaddressed."
 â”‚
 â–¼
Review includes an "Intent Check" section:
  âœ… Criterion 1: Satisfied (line 42 handles this)
  âŒ Criterion 3: Not addressed â€” no input validation added
  âš ï¸  Criterion 2: Partially â€” error case missing
```

**Integration targets:**
- GitHub Issues (via existing Octokit)
- Jira (Phase 3 ticket adapter already stubbed in `src/adapters/ticket/jira.ts`)
- Linear (also stubbed)
- Azure Boards

---

## P2: Production Metadata Injection

**The problem:** AI reviewers have no awareness of runtime reality. They can't know that a function handles 10M requests/hour, that this codepath caused the July outage, or that this file changes 5 times a week and is high-churn. Human senior engineers carry this institutional knowledge and calibrate review depth accordingly.

**The fix:** Feed lightweight production signals into review context so the LLM can adjust its review focus.

**Signal sources (cheapest to most complex):**

| Signal | Source | Value |
|--------|--------|-------|
| File change frequency | `git log --follow` | Flag high-churn files for extra scrutiny |
| File age / last touched | `git log` | Warn when ancient untested code is modified |
| Test coverage delta | Coverage report diff | Flag coverage regressions |
| Recent incident tags | PagerDuty / Opsgenie API | "This module caused 2 incidents in the last 90 days" |
| Error rate on changed endpoints | Datadog / CloudWatch | Flag changes to high-error-rate paths |
| Performance baseline | APM data | Warn when a hot-path function is changed |

**Minimal viable implementation:** Start with git-derived signals only (zero external dependencies):

```typescript
// In context/builder.ts
const fileSignals = await getFileSignals(diff.files, gitClient);
// Returns: { path, commitCount30d, daysSinceLastChange, authorCount }
// Injected into prompt: "âš ï¸ src/payments/charge.ts â€” 23 changes in last 30 days, 8 authors"
```

---

## P2: TypeScript Type-Aware Reviews

Use the TypeScript Compiler API (`ts.createProgram`) to extract type information, diagnostics, and function signatures, then inject this context into the review prompt for richer analysis.

```
ts.createProgram() â†’ TypeChecker â†’ getTypeAtLocation()
     â”‚
     â–¼
Extract types, diagnostics, function signatures
     â”‚
     â–¼
Inject into review prompt â†’ Type-aware LLM review
```

**What this enables:**
- Catch type errors the LLM can't see from diffs alone
- Understand overloaded function signatures
- Flag breaking changes to exported types

---

## P2: Codebase Embeddings (Context Awareness)

Chunk the codebase by function/class, generate embeddings via Vercel AI SDK `embedMany()`, and store them in a vector database (Qdrant). During review, retrieve semantically similar code patterns to enrich the review context.

```
Codebase â†’ Chunker (function/class) â†’ embedMany() â†’ Qdrant
     â”‚
     â–¼
During review â†’ Query similar patterns â†’ Inject into context
```

**What this enables:**
- "This pattern is used differently in 3 other places"
- Cross-file consistency checks
- Detecting duplication across the codebase

---

## P3: Multi-language LSP + Impact Analysis

### LSP Servers

| Language | LSP Server |
|----------|------------|
| TypeScript | `ts.createProgram()` |
| Python | Pyright / Pylance |
| Go | gopls |
| Rust | rust-analyzer |
| Java | jdtls |

### Impact Analysis

- Find all dependents of changed functions/classes
- Detect breaking API changes
- Suggest related files that may need updates
- Generate call graphs for affected code paths

---

## P3: Cross-Repository Impact Analysis

**The problem:** When a shared library, interface, or DTO is changed, all downstream services that depend on it are silently broken. No AI reviewer catches this because they only see the repo they've been given access to. This is structurally undetectable by diff-only tools.

**The fix:** Maintain a live dependency graph across repos. When a change touches a shared export, automatically surface all downstream consumers and flag specific incompatibilities.

```
Change: UserDTO.email field renamed to UserDTO.emailAddress
 â”‚
 â–¼
Dependency graph query:
  "which repos import UserDTO?"
  â†’ service-b, service-c, mobile-api, admin-panel
 â”‚
 â–¼
For each dependent repo:
  Fetch usages of UserDTO.email
  Flag: "service-b/src/user.service.ts:42 â€” references .email which no longer exists"
 â”‚
 â–¼
Review comment on the original PR:
  âš ï¸ Breaking change detected â€” 3 downstream consumers affected
```

---

## v2 Architecture Target

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     CI/CD Trigger or Webhook                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚
                               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        PR Event Handler                             â”‚
â”‚              â€¢ Incremental Diff Analyzer                            â”‚
â”‚              â€¢ Comment Manager (post/reply/resolve)                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â–¼                         â–¼                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Static       â”‚      â”‚  Context Builder     â”‚    â”‚  Institutional    â”‚
â”‚ Analysis     â”‚      â”‚                      â”‚    â”‚  Memory Store     â”‚
â”‚ Layer (P1)   â”‚      â”‚ â€¢ Diff + file sigs   â”‚    â”‚                   â”‚
â”‚              â”‚      â”‚ â€¢ Ticket/intent (P2) â”‚    â”‚ â€¢ Team decisions  â”‚
â”‚ â€¢ tsc        â”‚      â”‚ â€¢ Production signals â”‚    â”‚ â€¢ Dismissed rules â”‚
â”‚ â€¢ ESLint     â”‚      â”‚ â€¢ Embeddings (P2)    â”‚    â”‚ â€¢ Custom rules    â”‚
â”‚ â€¢ Semgrep    â”‚      â”‚ â€¢ Type info (P2)     â”‚    â”‚                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â”‚                         â”‚                         â”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        LLM Backend (Vercel AI SDK)                  â”‚
â”‚   "Triage and explain static findings. Add issues static tools      â”‚
â”‚    cannot detect. Score confidence 0â€“1 for each comment."           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚
                               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   Precision Filter (P1)                             â”‚
â”‚          Drop comments below confidence threshold                   â”‚
â”‚          If nothing remains â†’ post "No significant issues found"    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚
                               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        Comment Manager                              â”‚
â”‚              â€¢ Deduplication + version claim filter                 â”‚
â”‚              â€¢ Post inline comments                                 â”‚
â”‚              â€¢ Reply to threads                                     â”‚
â”‚              â€¢ Update checkpoint                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```
