# Docker Compose

The recommended way to self-host AgnusAI. The stack includes the API server, Postgres + pgvector, and Traefik reverse proxy. Ollama is **optional** — skip it if you use a cloud LLM/embedding provider.

## One-Command Setup (Recommended)

The installer handles everything interactively:

```bash
git clone https://github.com/ivoyant-eng/AgnusAi.git
cd AgnusAi
bash install.sh
```

The installer will prompt for your LLM provider, API keys, embedding provider, and whether to run Ollama in Docker or on your host — then launches `docker compose up --build` automatically.

## Manual Setup

```bash
cp .env.example .env
# Edit .env — see sections below
docker compose up --build
```

## What Starts by Default

| Service | Port | Description |
|---------|------|-------------|
| `agnus` | 3000 | Fastify API server (via Traefik on :80) |
| `postgres` | 5432 | Postgres 16 + pgvector |
| `traefik` | 80/443/8080 | Reverse proxy / gateway |

**Ollama is not started by default.** It's added by `docker-compose.override.yml` when you run the installer and choose "Ollama in Docker", or you can add it manually (see below).

## Adding Ollama in Docker (Optional)

If you want Ollama managed by Docker Compose, create `docker-compose.override.yml`:

```yaml
services:
  ollama:
    image: ollama/ollama:latest
    volumes:
      - ollama-data:/root/.ollama
    ports:
      - "11434:11434"
    restart: unless-stopped
    networks:
      - agnus-network

  agnus:
    depends_on:
      - ollama

volumes:
  ollama-data:
```

Then set in `.env`:
```env
OLLAMA_BASE_URL=http://ollama:11434/v1
EMBEDDING_BASE_URL=http://ollama:11434   # if using Ollama for embeddings
```

And pull models after startup:
```bash
docker compose exec ollama ollama pull qwen3.5:397b-cloud
docker compose exec ollama ollama pull qwen3-embedding:0.6b   # for embeddings
```

For NVIDIA GPU, add to the `ollama` service:
```yaml
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
```

## Using Ollama on Host (Alternative)

If Ollama is already running on the host machine:

```env
OLLAMA_BASE_URL=http://host.docker.internal:11434/v1
EMBEDDING_BASE_URL=http://host.docker.internal:11434   # if using Ollama for embeddings
```

## Environment Variables

Key variables the compose file reads from `.env`:

```env
# Secrets (auto-generated by install.sh, or generate manually)
WEBHOOK_SECRET=...        # openssl rand -hex 32
SESSION_SECRET=...        # openssl rand -hex 32
JWT_SECRET=...            # openssl rand -hex 32

# Admin
ADMIN_EMAIL=admin@example.com
ADMIN_PASSWORD=changeme   # change this

# LLM — pick one provider
LLM_PROVIDER=ollama       # ollama | openai | claude | azure | custom
LLM_MODEL=qwen3.5:397b-cloud

# Ollama (if LLM_PROVIDER=ollama)
OLLAMA_BASE_URL=http://host.docker.internal:11434/v1

# OpenAI (if LLM_PROVIDER=openai)
OPENAI_API_KEY=sk-...

# Claude (if LLM_PROVIDER=claude)
ANTHROPIC_API_KEY=sk-ant-...

# Azure OpenAI (if LLM_PROVIDER=azure)
AZURE_OPENAI_ENDPOINT=https://your-resource.cognitiveservices.azure.com/...
AZURE_OPENAI_API_KEY=...

# Embeddings — optional, enables deep review depth
EMBEDDING_PROVIDER=       # ollama | openai | google | azure | http (leave blank to disable)
EMBEDDING_MODEL=
EMBEDDING_BASE_URL=
EMBEDDING_API_KEY=

REVIEW_DEPTH=standard     # fast | standard | deep

# VCS
GITHUB_TOKEN=ghp_...

# Public URL (for webhook callbacks and feedback links)
PUBLIC_URL=http://your-server:3000
```

## Register Your First Repo

```bash
# Save session cookie
curl -c /tmp/agnus.txt -X POST http://localhost:3000/api/auth/login \
  -H "Content-Type: application/json" \
  -d '{"email":"admin@example.com","password":"changeme"}'

# Register repo
curl -b /tmp/agnus.txt -X POST http://localhost:3000/api/repos \
  -H "Content-Type: application/json" \
  -d '{
    "repoUrl": "https://github.com/owner/repo",
    "platform": "github",
    "token": "ghp_...",
    "repoPath": "/repos/owner-repo",
    "branches": ["main"]
  }'
```

## Data Persistence

All data persists in named Docker volumes:

- `postgres-data` — symbol graph, edges, embeddings, snapshots
- `ollama-data` — downloaded model weights (only if using Ollama in Docker)

To reset everything: `docker compose down -v`

## Health Check

```bash
curl http://localhost:3000/health
# {"status":"ok","timestamp":"..."}
```

## Updating

```bash
git pull
docker compose up --build
```

Schema migrations run automatically on startup (idempotent).
